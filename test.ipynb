{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 4 pts resample from data\n",
    "+ 4 pts resampling done with replacement\n",
    "+ 4 pts take several sets of resamplings\n",
    "+ 2 pts implies repeated resampling, but does not mention explicitly\n",
    "+ 4 pts determine the coeﬃcients from the line of best fit for each resampling\n",
    "+ 2 pts mentions calculating a metric from each resampling, but does not specify OR metric calculated from each resampling, but incorrect one\n",
    "+ 4 pts gather all lines of best fit to gauge reliability (plot all lines together or plot histogram of coeﬃcients)\n",
    "+ 2 pts mentions comparing coeﬃcients to coeﬃcient for original data\n",
    "+ 4 pts incorrect description of bootstrapping, but mentions generation of a confidence interval around line of best fit\n",
    "+ 0 pts incorrect or no attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_response = \"Bootstrapping creates a confidence interval of possible lines of best fit. 1) Obtain a sample data set from observation/experiment with replacement. 2) Generate a new randomized data set from the sample data set (step 1) that has the same number of observations. ex: sample has 1000 points, new data set has 1000 based on this. 3) Repeat step 2 multiple times (>10) and determine the line of best fit for each scatter plot. 4) Place these lines on a scatter plot and a range of linear lines should be seen, this will determine if the original line is within the range of a reliable fit (confidence interval of linear model lines)\"\n",
    "\n",
    "rubric_component = \"Implies repeated resampling, but does not mention explicitly\"\n",
    "\n",
    "context = \"Describe, step-by-step, the process of bootstrapping to estimate the reliability of coefficients fit by a linear model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='gemma2', messages=[\n",
    "  {\n",
    "      'role': 'system',\n",
    "      'content': f\"You are a biology grading assistant. Your task is to evaluate student responses to determine if the rubric component is explicitly addressed{', taking into account the information provided to the student in the context section' if context else ''}, regardless of its factual accuracy. Do not use your own knowledge to judge the correctness of the rubric component or the student's response. Your role is to check if the student's response matches what the rubric asks for, even if the rubric component itself contains inaccuracies.\",\n",
    "  },\n",
    "  {\n",
    "      'role': 'user',\n",
    "      'content': f\"\"\"\n",
    "        The rubric component is: '{rubric_component}'.\n",
    "        The student response is: '{student_response}'.\n",
    "        {'The context is: ' + f\"'{context}'\" if context else ''}\n",
    "\n",
    "        Your task is to evaluate if the student's response demonstrates understanding of the core concepts in the rubric component. Walk through your analysis step by step:\n",
    "\n",
    "        1. ANALYSIS OF RUBRIC:\n",
    "        - Break down the key concepts and requirements from the rubric component\n",
    "        - What are the essential ideas that must be demonstrated?\n",
    "\n",
    "        2. ANALYSIS OF RESPONSE:\n",
    "        - Identify relevant portions of the student's response\n",
    "        - Look for both direct statements and implied understanding\n",
    "        - Consider alternative phrasings and terminology that convey the same concepts\n",
    "\n",
    "        3. EVIDENCE EVALUATION:\n",
    "        - Match concepts from the rubric to evidence in the response\n",
    "        - Consider semantic equivalence and contextual meaning\n",
    "        - Note any missing or misunderstood concepts\n",
    "        - Assess if implied or indirect demonstrations of knowledge are sufficient\n",
    "\n",
    "        4. FINAL DETERMINATION:\n",
    "        After completing your analysis, provide your final evaluation of whether the rubric component is satisfied.\n",
    "\n",
    "        Important evaluation guidelines:\n",
    "        - Focus on conceptual understanding rather than exact wording matches\n",
    "        - Students may use different terminology to express the same ideas\n",
    "        - Consider the context of the field when evaluating semantic equivalence\n",
    "        - Look for evidence of understanding rather than perfect articulation\n",
    "        - If a concept is implied through a clear description of its implementation or consequences, consider it present\n",
    "\n",
    "        Examples of semantic equivalence:\n",
    "        - \"Gather all lines\" = \"Compile all lines\" = \"Collect all lines\" = \"Put together all lines\"\n",
    "        - \"Gauge reliability\" = \"Assess reliability\" = \"Evaluate reliability\" = \"Determine reliability\"\n",
    "        - \"Plot together\" = \"Display together\" = \"Show together\" = \"Visualize together\"\n",
    "\n",
    "        Your response MUST:\n",
    "        1. Show your complete thought process through steps 1-3\n",
    "        2. Only provide your final determination (satisfied/unsatisfied) AFTER showing this analysis\n",
    "        3. End with the word \"EVALUATION:\" followed by your clear Yes/No determination and brief summary of why\n",
    "\n",
    "        \"\"\"\n",
    "  }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content = response.message.content\n",
    "\n",
    "type(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_json_schema = \"\"\"{\n",
    "    \"rubricComponentSatisfied\": <true/false based on Yes/No>,\n",
    "    \"explanation\": <concise summary of key reasoning>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "evaluator_json_schema = \"\"\"{\n",
    "    \"gradersAgree\": <true/false>\n",
    "    \"consensusEvaluation\": <'Yes'/'No'/'No consensus reached'>\n",
    "    \"explanation\": <concise summary of key reasoning>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonified: ChatResponse = chat(model='gemma2', messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': f\"\"\"            \n",
    "            You are a JSON formatter. Your task is to convert a detailed grading analysis into a standardized JSON response.\n",
    "\n",
    "            The input will contain a complete analysis followed by a final evaluation. Your job is to:\n",
    "            1. Identify the final Yes/No determination\n",
    "            2. Extract the key reasoning from the analysis\n",
    "            3. Format these into valid JSON\n",
    "\n",
    "            Rules for JSON creation:\n",
    "            1. Only include the final determination, not intermediate thoughts\n",
    "            2. Summarize the key reasoning in 2-3 clear sentences\n",
    "            3. Format the response exactly as:\n",
    "\n",
    "            {grader_json_schema}\n",
    "\n",
    "            Format requirements:\n",
    "\n",
    "            - Use exact property names as shown\n",
    "            - Use true/false (not \"true\"/\"false\" or Yes/No)\n",
    "            - Keep explanation clear and concise\n",
    "            - Ensure valid JSON syntax\n",
    "            - Do not include any other fields\n",
    "            - Do not include any text before or after the JSON\n",
    "\n",
    "            \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': response_content\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsonified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jsonified_content \u001b[38;5;241m=\u001b[39m \u001b[43mjsonified\u001b[49m\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(jsonified_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonified' is not defined"
     ]
    }
   ],
   "source": [
    "jsonified_content = jsonified.message.content\n",
    "\n",
    "print(jsonified_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsonified_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loaded_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mjsonified_content\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mtype\u001b[39m(loaded_json)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(loaded_json)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonified_content' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "loaded_json = json.loads(jsonified_content)\n",
    "\n",
    "type(loaded_json)\n",
    "\n",
    "print(loaded_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader\n"
     ]
    }
   ],
   "source": [
    "from models.response_models.grader_response_model import GraderResponseModel\n",
    "from models.response_models.evaluator_response_model import EvaluatorResponseModel\n",
    "\n",
    "validation_model = GraderResponseModel(rubricComponentSatisfied=True,explanation=\"Test\")\n",
    "# validation_model = EvaluatorResponseModel(gradersAgree=True,consensusEvaluation=\"No\",explanation=\"Test\")\n",
    "\n",
    "if type(validation_model) is GraderResponseModel:\n",
    "    print(\"Grader\")\n",
    "elif type(validation_model) is EvaluatorResponseModel:\n",
    "    print(\"Evaluator\")\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(validation_model, GraderResponseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prompt_models.query_model import QueryModel\n",
    "\n",
    "import importlib\n",
    "import utils.query_large_language_model\n",
    "import utils.initiate_debate\n",
    "importlib.reload(utils.query_large_language_model)\n",
    "importlib.reload(utils.initiate_debate)\n",
    "from utils.initiate_debate import initiate_debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"Round {len(round_list) + 1} initiated.\": 'Round 1 initiated.'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m QueryModel(rubric_component\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplies repeated resampling, but does not mention explicitly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                     student_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBootstrapping creates a confidence interval of possible lines of best fit. 1) Obtain a sample data set from observation/experiment with replacement. 2) Generate a new randomized data set from the sample data set (step 1) that has the same number of observations. ex: sample has 1000 points, new data set has 1000 based on this. 3) Repeat step 2 multiple times (>10) and determine the line of best fit for each scatter plot. 4) Place these lines on a scatter plot and a range of linear lines should be seen, this will determine if the original line is within the range of a reliable fit (confidence interval of linear model lines)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe, step-by-step, the process of bootstrapping to estimate the reliability of coefficients fit by a linear model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m                     )\n\u001b[0;32m----> 6\u001b[0m \u001b[43minitiate_debate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/summate-ai-local/utils/initiate_debate.py:34\u001b[0m, in \u001b[0;36minitiate_debate\u001b[0;34m(query, grader1_model, grader2_model, evaluator_model)\u001b[0m\n\u001b[1;32m     30\u001b[0m ic(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(round_list)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m initiated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m grader1_start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 34\u001b[0m grader1_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_large_language_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrader1_message_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrader1_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mvalidation_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraderResponseModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m grader1_response_formatted \u001b[38;5;241m=\u001b[39m ResponseModel(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m                                         model\u001b[38;5;241m=\u001b[39mgrader1_model,\n\u001b[1;32m     40\u001b[0m                                         content\u001b[38;5;241m=\u001b[39mgrader1_response,\n\u001b[1;32m     41\u001b[0m                                         time_requested\u001b[38;5;241m=\u001b[39mgrader1_start_time,\n\u001b[1;32m     42\u001b[0m                                         time_completed\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[1;32m     44\u001b[0m ic(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrader 1 response has been received.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/summate-ai-local/utils/query_large_language_model.py:32\u001b[0m, in \u001b[0;36mquery_large_language_model\u001b[0;34m(query, model, validation_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m message_to_be_jsonified \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     31\u001b[0m json_message_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m json_message_models \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_to_jsonifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_to_be_jsonified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_to_be_jsonified\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m json_message_models:\n\u001b[1;32m     35\u001b[0m     json_message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/summate-ai-local/utils/message_generation.py:127\u001b[0m, in \u001b[0;36mprompt_to_jsonifier\u001b[0;34m(validation_model, message_to_be_jsonified)\u001b[0m\n\u001b[1;32m    125\u001b[0m         response_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     schema \u001b[38;5;241m=\u001b[39m grader_json_schema \u001b[38;5;28;01mif\u001b[39;00m response_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m evaluator_json_schema\n\u001b[1;32m    131\u001b[0m     grader_json_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrubricComponentSatisfied\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: <true/false based on Yes/No>,\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: <concise summary of key reasoning>\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m    }\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = QueryModel(rubric_component=\"Implies repeated resampling, but does not mention explicitly\",\n",
    "                    student_response=\"Bootstrapping creates a confidence interval of possible lines of best fit. 1) Obtain a sample data set from observation/experiment with replacement. 2) Generate a new randomized data set from the sample data set (step 1) that has the same number of observations. ex: sample has 1000 points, new data set has 1000 based on this. 3) Repeat step 2 multiple times (>10) and determine the line of best fit for each scatter plot. 4) Place these lines on a scatter plot and a range of linear lines should be seen, this will determine if the original line is within the range of a reliable fit (confidence interval of linear model lines)\",\n",
    "                    context=\"Describe, step-by-step, the process of bootstrapping to estimate the reliability of coefficients fit by a linear model.\"\n",
    "                    )\n",
    "\n",
    "initiate_debate(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "        \"gradersAgree\": True,\n",
    "        \"consensusEvaluation\": \"Yes\",\n",
    "        \"explanation\": \"The student describes repeatedly generating new datasets and performing calculations on each, clearly implying bootstrapping despite not using the term. This fulfills the rubric's requirement of demonstrating the concept through description.\" \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
